---
title: Genie Game Results — Followup
date: "`r Sys.Date()`"
author: "fredcallaway"
output:
  rmdformats::robobook:
    code_folding: hide
    self_contained: true
---


In [the last set of results](http://fredcallaway.com/reports/autonomy/genie-22-01-21.html),
we had a very weak manipulation effect. To test whether the weakened effect was due to
changes we made from earlier (more successful) pilots, we tried reverting the key changes.
Concretely, the reversions are:

- using hand-selected outcome sets instead of those based on the accessibility norming study
- not asking participants to report outcomes in consideration order
- removing text emphasizing that the slider should be used in a consistent way throughout the experiment

For the last point, we currently have

> Breaking character for a bit, we know you've seen a slider before. But it's
> really important for our research that you use _our_ slider in a consistent way
> throughout the experiment.

and the last set of results instead had

> Breaking character for a bit, we know you've seen a slider before. But
> **it's really important for our research that you
> use the slider in a consistent way throughout the experiment, *even
> in different rounds.*** So, if you rate situation A in the first
> round as +3 and situation B in the third round as +4, it means
> you'd rather be in situation B than situation A.


```{r setup, include=FALSE}
source("base.r")
library(moments)
library(scales)
library(infer)

VERSIONS = c('v4.0', 'v4.0B')
# VERSIONS = c('v3.1')

load_data = function(type) {
    VERSIONS %>% 
    map(~ 
        read_csv(glue('../data/{.x}/{type}.csv'), col_types = cols()) %>% 
        mutate(version = .x)
     ) %>% 
    bind_rows
}

considered_colors = scale_colour_manual(values=c(
    "#3DD93D",
    "#666666"
), aesthetics=c("fill", "colour"), name="")

control_colors = scale_colour_manual(values=c(
    "#31ADF4",
    "#F5CE47"
), aesthetics=c("fill", "colour"))

wrap_scenario = list(
    facet_wrap(~scenario, dir="v", ncol=4),
    theme(strip.text.x = element_text(size=12), legend.position="top")
)
```

# Exclusions

```{r load_data}
full_pdf = load_data('participants') %>% 
    filter(completed) %>% 
    select(-c(consideration_time, reflection_time, n_considered, n_unconsidered, version, bonus, completed)) %>%
    mutate(
        obligatory_check = factor(if_else(comp_radio, 'incorrect', 'correct', 'empty'),
                                  levels=c('empty', 'incorrect', 'correct')),
    )

full_df = load_data('evaluation') %>% 
    rename(scenario = prompt_id) %>% 
    mutate(scenario = tolower(scenario))

full_consideration = load_data('consideration') %>% 
    rename(scenario = prompt_id) %>% 
    mutate(scenario = tolower(scenario))

full_slider = load_data('slider_check')

slider_check = full_slider %>% 
    mutate(prompt = word(prompt)) %>% 
    pivot_wider(names_from=prompt, values_from=response) %>% 
    mutate(
        p1 = Breaking < Stubbing,
        p2 = Stubbing < Finding,
        p3 = Finding < Winning,
        pass = p1 & p2 & p3
    )

# slider_check %>% summarise(across(-wid, mean))

# table(full_pdf$obligatory_check)
pdf = full_pdf %>% 
    filter(obligatory_check == "correct") %>% 
    left_join(slider_check) %>% 
    filter(pass) %>%
    mutate(
        control = factor(control, levels=c("low", "high"))
    )

pdf2 = select(pdf, wid, control)

consideration = full_consideration %>%
    right_join(pdf2)  %>% 
    filter(scenario != "practice") %>%
    filter(outcome %nin% c("NONE", "UNK")) %>%
    group_by(wid, scenario)  %>% 
    distinct(outcome) %>%   # EXCLUSION
    mutate(order = row_number())

df = full_df %>% 
    right_join(pdf2) %>% 
    filter(scenario != "practice") %>% 
    left_join(transmute(consideration, wid, scenario, outcome, order, considered=T)) %>% 
    replace_na(list(considered=F)) %>% 
    group_by(wid) %>% 
    mutate(
        evaluation_z = zscore(evaluation),
        abs_eval = abs(evaluation),
        abs_eval_z = zscore(abs_eval),
    ) %>% 
    ungroup()

raw_acc = read_csv('../data/acc-v2.0/accessibility.csv', col_types = cols())

# accessibility is the proportion of participants who listed each outcome
# when prompted with its category
accessibility = raw_acc %>% 
    filter(cat_id != "EUROCITIES") %>%
    filter(outcome != "UNK") %>%
    count(cat_id,outcome) %>% 
    mutate(accessibility = n / length(unique(raw_acc$wid))) %>% 
    select(-n)

df = left_join(df, accessibility) %>% 
    filter(!is.na(accessibility))
    # replace_na(list(accessibility=0))
# stopifnot(df %>% group_by(wid, scenario) %>% filter(sum(considered)!=1) %>% nrow == 0)

stopifnot(
    full_df %>% 
    filter(scenario != "practice" & outcome == "UNK") %>% 
    nrow == 0
)

df = df %>% 
    arrange(wid, scenario, order) %>% 
    group_by(wid, scenario) %>% 
    mutate(trial_id= cur_group_id()) %>% 
    ungroup()

df %>% write_csv("../data/processed.csv")
```

- `r nrow(full_pdf)` participants recruited.
- `r sum(full_pdf$obligatory_check != "correct")` failed the comprehension check.
- `r sum(!slider_check$pass)` participants failed the slider check.
- `r nrow(pdf)` passed both checks and are included in the analysis.


# Proportion of good/bad/neutral

```{r}
ebins = df %>% 
    group_by(evaluation) %>% 
    filter(!is.na(accessibility)) %>% 
    summarise(acc=sum(accessibility)) %>% 
    mutate(prop=acc/sum(acc)) %>% 
    mutate(ebin=cut(cumsum(prop), c(0, 1/3, 2/3, 1.01), labels=c("bad", "neutral", "good"))) %>% 
    select(evaluation, ebin)

df %>% 
    left_join(ebins) %>% 
    filter(considered)  %>% 
    ggplot(aes(ebin, y=..prop.., group=control,fill=control)) + 
    geom_bar(position="dodge", alpha=0.7) +
    geom_hline(yintercept=1/3, linetype="solid") +
    control_colors +
    labs(x="value bin", y="proportion of considered outcomes")
```

## Tests

```{r}
baselines = df %>%
    filter(!is.na(accessibility)) %>% 
    group_by(evaluation) %>% 
    summarise(acc=sum(accessibility))  %>% 
    mutate(prop=acc/sum(acc))

X = baselines %>% 
    mutate(ebin=cut(cumsum(prop), c(0, 1/3, 2/3, 1.01), labels=c("bad", "neutral", "good"))) %>% 
    select(evaluation, ebin) %>% 
    right_join(df) %>% 
    filter(considered) %>%
    count(control, ebin) %>% 
    pivot_wider(names_from=ebin, values_from=n) %>%
    mutate(total=bad+neutral+good)

low =  X %>% filter(control=="low")
high = X %>% filter(control=="high")

cat(
    "\n\nlow-control good > chance:",
    pval(prop.test(low$good, low$total, 1/3, 'greater')$p.value),
    "\n\nlow-control bad > chance:",
    pval(prop.test(low$bad, low$total, 1/3, 'greater')$p.value),
    "\n\nlow-control neutral < chance:",
    pval(prop.test(low$neutral, low$total, 1/3, 'less')$p.value),
    "\n\nhigh-control good > chance:",
    pval(prop.test(high$good, high$total, 1/3, 'greater')$p.value),
    "\n\nhigh-control bad > chance:",
    pval(prop.test(high$bad, high$total, 1/3, 'less')$p.value),
    "\n\nhigh-control neutral < chance:",
    pval(prop.test(high$neutral, high$total, 1/3, 'less')$p.value),
    "\n\nlow bad > high bad:",
    pval(prop.test(X$bad, X$total, alternative='greater')$p.value),
    "\n\nhigh good > low good:",
    pval(prop.test(X$good, X$total, alternative='less')$p.value)
)
```

OK, so we definitely have a much stronger manipulation effect; that's great.
But the low-control group appears to be selectively considering *bad* outcomes,
rather than extreme outcomes. This is especially puzzling given the last
set of results, where people in both conditions selectively considered good outcomes.

# Regression "check" plot

```{r, fig.width=8, fig.height=4}
# BASELINE = mean(df$evaluation)

relu = function(x) {
    if_else(x < 0, 0, x)
}
p1 = df %>% 
    mutate(considered_first = as.numeric(considered & order==1)) %>% 
    ggplot(aes(evaluation, considered_first, color=control)) +
    stat_summary_bin(fun.data=mean_cl_boot, bins=5, alpha=0.5, 
                     position=position_dodge(width=.5)) +
    stat_smooth(geom="line", size=0.8, linetype = "dotted", alpha=0.5) +
    # geom_smooth(se=F, method=lm, formula=y ~ x + abs(x), alpha=0.1) +
    geom_smooth(se=F, method=glm, 
        formula=y ~ x + abs(x),
        method.args = list(family = "binomial"),
        alpha=0.1) +
    ylab("consideration probability") +
    control_colors

p2 = df %>% 
    ggplot(aes(evaluation, considered - accessibility, color=control)) +
    stat_summary_bin(fun.data=mean_cl_boot, bins=5, alpha=0.5, 
                     position=position_dodge(width=.5)) +
    stat_smooth(geom="line", size=0.8, linetype = "dotted", alpha=0.5) +
    geom_smooth(se=F, method=lm, formula=y ~ abs(x) + x, alpha=0.1) +
    # geom_smooth(se=F, method=lm, formula=y ~ exp(0.2*x) + abs(x), alpha=0.1) +
    ylab("relative consideration probability") +
    control_colors

p1 + p2 + plot_layout(guides = "collect")
```

```{r, fig.height=2, cache=T, dependson=load_data}
consideration_model = df %>% 
    mutate(
        accessibility = zscore(accessibility), 
        evaluation = zscore(evaluation),
        control_high = 1*(control == "high"),
        control_low = 1*(control == "low"),

    ) %>% 
    glmer(considered ~ accessibility + evaluation + abs(evaluation) +
        control_low : abs(evaluation) + control_high : evaluation +
        (accessibility + evaluation + abs(evaluation) | wid),
     family=binomial, data=.)

plot_coefs(consideration_model, omit.coefs=c("controlhigh", "(Intercept)"), colors="black") #plot
summ(consideration_model)
```

This analysis yields somewhat different conclusions because absolute value
seems to matter in both conditions. Note that I've gone back to using signed
value as a predictor (rather than relu) because we have a negative effect of
signed value in the low-control condition.

# Value distributions

This is here for completeness; feel free to skip.

Gray is the accessibility data. Blue and yellow are outcomes considered by each group.

```{r, fig.width=7.5, fig.height=4.5}

baselines = df %>%
    group_by(scenario, evaluation) %>% 
    summarise(acc=sum(accessibility))  %>% 
    group_by(scenario) %>% 
    mutate(prop=acc/sum(acc))

df %>% 
    filter(considered) %>% 
    count(control, scenario, evaluation) %>% 
    group_by(control, scenario) %>% 
    mutate(prop=n/sum(n)) %>% 
    ggplot(aes(evaluation, prop)) + 
        geom_bar(data=baselines, stat="identity", alpha=1, fill="gray50", position="identity") +
        geom_bar(aes(fill=control), stat="identity", alpha=0.7, position="identity") +
        control_colors + wrap_scenario + 
        labs(fill="considered by", y="proportion")
```

```{r, fig.width=7.5}
baselines = df %>%
    group_by(evaluation) %>% 
    summarise(acc=sum(accessibility))  %>% 
    mutate(prop=acc/sum(acc))

df %>% 
    filter(considered) %>% 
    count(control, evaluation) %>% 
    group_by(control) %>% 
    mutate(prop=n/sum(n)) %>% 
    ggplot(aes(evaluation, prop)) + 
        geom_bar(data=baselines, stat="identity", alpha=1, fill="gray50", position="identity") +
        geom_bar(aes(fill=control), stat="identity", alpha=0.7, position="identity") +
        control_colors + facet_wrap(~control) +
        labs(fill="considered by", y="proportion") + 
        theme(legend.position="none")
```

# Relative proportions

I think this plot might be the most informative. It shows the frequency of
each value among considered outcomes, minus the frequency we would expect from
accessibility alone.

```{r, fig.width=7.5}
baselines = df %>%
    group_by(evaluation) %>% 
    summarise(acc=sum(accessibility))  %>% 
    mutate(prop=acc/sum(acc))

df %>% 
    filter(considered) %>% 
    count(control, evaluation) %>% 
    group_by(control) %>% 
    mutate(prop=n/sum(n)) %>% 
    left_join(transmute(baselines, evaluation, baseline=prop)) %>% 
    ggplot(aes(evaluation, prop-baseline)) + 
        geom_bar(aes(fill=control), stat="identity", alpha=0.7, width=.7, position="dodge") +
        control_colors +
        labs(y="relative proportion")
```

From this, I conclude that
- high control participants consider fairly good things (+5 and above)
  and avoid considering the very worst things
- low control participants consider the very worst things and
  avoid considering anything positive

These results are actually consistent with our predictions (in particular,
sampling extreme outcomes under low control), with two additional
assumptions:
- overall, the value distribution is skewed negative, such that there are more
  terrible outcomes than amazing outcomes
- people are interpreting -10/10 to be something like the .05 and .95 percentiles
  of value

The consequence of these two assumptions is that -10 outcomes are actually
more extreme than +10 outcomes. Therefore low-control participants should be
more likely to sample the -10s.

# Modeling

## Model comparison

- loo is an estimate of predictive likelihood; it will be close to AIC / -2.
- d_loo is the difference in loo from the best model
- dse is the standard error of that difference
- weight "can be loosely interpreted as the probability of each model
  (among the compared model) given the data"

```{r}
read_csv("../model/results/comparison.csv") %>% 
    select(name, loo, d_loo, dse, weight) %>% 
    mutate(across(-name, signif, 4)) %>% 
    kable
```

Softmax alone (with different temperature by condition) performs just as well
as the softmax+UWS model. Note: I don't understand how/why "weight" can
disagree with the difference in LOO. It might depend on which other models
are in the comparison.

## Difference in β

```{r}
chain = read_csv('../model/results/chain.csv')

chain %>% 
    select(β_low, β_high) %>% 
    pivot_longer(c(β_low, β_high), names_to="param", values_to="value", names_prefix="") %>% 
    mutate(param = factor(param, levels=c("β_low", "β_high"))) %>% 
    ggplot(aes(value, fill=param, color=param)) +
    geom_density(alpha=0.2) + control_colors

p_higher = mean(chain$β_high > chain$β_low)
```

Really clear manipulation effect. Note that the temperature is *negative* in
the low control condition. That corresponds to a traditional softmax over
negative value (specifically considering bad things).

## Posterior predictive check(s)

Here we are recreating plot(s) using simulations from the joint model.

```{r}
ppc = read_csv("../model/results/ppc.csv") %>% 
    pivot_longer(c(low, high), names_to="control", values_to="n", names_prefix="") %>% 
    mutate(control = factor(control, levels=c("low", "high")))

ppc %>% 
    left_join(ebins) %>% 
    group_by(control,ebin) %>% 
    summarise(n=sum(n)) %>% 
    group_by(control) %>%
    mutate(prop = n / sum(n)) %>% 
    ggplot(aes(ebin, y=prop, group=control,fill=control)) + 
    geom_bar(position="dodge", stat="identity", alpha=0.7) +
    geom_hline(yintercept=1/3, linetype="solid") +
    control_colors +
    labs(x="value bin", y="proportion of considered outcomes")
```

Not bad!
